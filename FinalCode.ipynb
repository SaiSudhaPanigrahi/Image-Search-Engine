{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10000\n",
    "v_len = 1000\n",
    "test_size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in description txt (5 sentences each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_des_train():\n",
    "    train_path = './data/descriptions_train/' \n",
    "    des_train = []\n",
    "    for i in range(train_size):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        des = []\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                des.append(line.strip('\\n'))\n",
    "        des_train.append(des)\n",
    "    return des_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_train = read_des_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in tags and build diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tag_train():\n",
    "    train_path = './data/tags_train/' \n",
    "    tag_train = []\n",
    "    cat_list = []\n",
    "    sup_cat_list = []\n",
    "    for i in range(train_size):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        tag = ''\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                sup_cat, sub_cat = line.strip('\\n').split(':')\n",
    "                tag = tag + ' ' + sub_cat\n",
    "                cat_list.append(sub_cat)\n",
    "                sup_cat_list.append(sup_cat)\n",
    "        tag.strip()\n",
    "        tag_train.append(tag)\n",
    "    return tag_train, cat_list, sup_cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_train, category_list, sup_list = read_tag_train()\n",
    "category_list = list(set(category_list))\n",
    "sup_list = list(set(sup_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilized preprocessing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprop_description(desc_data):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    desc_words_bow = []\n",
    "    for descs in desc_data:\n",
    "        temp = ''\n",
    "        for i in range(len(descs)):\n",
    "            word_lower = descs[i].lower()\n",
    "            word_list = re.findall(r\"[a-z]+\", word_lower)\n",
    "            word_filter = [stemmer.stem(word) for word in word_list if word not in stopwords]\n",
    "            for j in range (len(word_filter)):\n",
    "                temp = temp + ' '+word_filter[j] \n",
    "        desc_words_bow.append(temp)\n",
    "    return desc_words_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Zhiling/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "desc_word_bow = preprop_description(desc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "cv = CountVectorizer(min_df = 15) # frequency threshold\n",
    "x_bow_train = cv.fit_transform(desc_word_bow)\n",
    "words_extract = np.array(cv.get_feature_names())\n",
    "transformer = TfidfTransformer()\n",
    "x_tf_train = transformer.fit_transform(x_bow_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf_train = pd.DataFrame(x_tf_train, columns = words_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a vector of word counts for the preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(vocabulary = category_list)\n",
    "tags_train_c = cv.fit_transform(tags_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_des_test():\n",
    "    train_path = './data/descriptions_test/' \n",
    "    des_test = []\n",
    "    for i in range(2000):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        des = []\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                des.append(line.strip('\\n'))\n",
    "        des_test.append(des)\n",
    "    return des_test\n",
    "\n",
    "desc_test = read_des_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_test_bow = preprop_description(desc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(vocabulary = words_extract)\n",
    "x_bow_test = cv.fit_transform(desc_test_bow)\n",
    "transformer = TfidfTransformer()\n",
    "x_test_tf = transformer.fit_transform(x_bow_test).toarray()\n",
    "x_test_tf = pd.DataFrame(x_test_tf, columns = words_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_tf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_pool5 = PCA(n_components=1224, svd_solver='auto') \n",
    "pca_pool5.fit(x_tf_train)\n",
    "train_PCA = pca_pool5.transform(x_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_PCA = pca_pool5.transform(x_test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature train data pool-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train_data = pd.read_csv(filepath_or_buffer=\"./data/features_train/features_resnet1000intermediate_train.csv\",header=None\n",
    ")\n",
    "ft_train_data[0] = ft_train_data[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "ft_train_sort = ft_train_data.sort_values(by=[0])\n",
    "ft_train = ft_train_sort[list(range(1,2049))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2049)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature test data pool-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_test_data = pd.read_csv(filepath_or_buffer=\"./data/features_test/features_resnet1000intermediate_test.csv\",header=None\n",
    ")\n",
    "ft_test_data[0] = ft_test_data[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "ft_test = ft_test_data[list(range(1,2049))].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Least Squares Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "pls_p5 = PLSRegression(n_components=1224) # 2048 for future approach\n",
    "pls_p5.fit(train_PCA, ft_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbs = NearestNeighbors(n_neighbors=20, metric='cosine').fit(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_pls = []\n",
    "for i in range(2000):\n",
    "    predict = pls_p5.predict(np.array([test_PCA[i]]))\n",
    "    distance, idx = nbs.kneighbors(predict)\n",
    "    predict_pls.append(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predict_tags = [] \n",
    "for i in range(80):\n",
    "    print i\n",
    "    y = tags_train_c[:,i]\n",
    "    clf = LinearRegression(n_jobs = -1) \n",
    "    clf.fit(x_tf_train, y)\n",
    "    tag = clf.predict(x_test_tf)\n",
    "    lr_predict_tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for row in (predict_pls):\n",
    "    img_num = ''\n",
    "    for val in row:\n",
    "        img_num = img_num + ' ' + (str(val) + \".jpg\")\n",
    "    img_list.append(img_num)\n",
    "idx = []\n",
    "for i in range(2000):\n",
    "    idx.append(str(i)+ \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.DataFrame(img_list, columns = [\"Top_20_Image_IDs\"])\n",
    "idx = pd.DataFrame(idx, columns = [\"Descritpion_ID\"])\n",
    "result = pd.concat([idx, images], axis=1)\n",
    "result.to_csv(\"pls_pcaP5_result.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# read in description txt (5 sentences each)\n",
    "def read_des_train():\n",
    "    train_path = './data/descriptions_train/' \n",
    "    des_train = []\n",
    "    for i in range(10000):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        des = []\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                des.append(line.strip('\\n'))\n",
    "        des_train.append(des)\n",
    "    return des_train\n",
    "\n",
    "\n",
    "desc_train = read_des_train()\n",
    "\n",
    "# read in tags\n",
    "def read_tag_train():\n",
    "    train_path = './data/tags_train/' \n",
    "    tag_train = []\n",
    "    cat_list = []\n",
    "    for i in range(10000):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        tag = ''\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                sup_cat, sub_cat = line.strip('\\n').split(':')\n",
    "                tag = tag + ' ' + sub_cat\n",
    "                cat_list.append(sub_cat)\n",
    "        tag.strip()\n",
    "        tag_train.append(tag)\n",
    "    return tag_train, cat_list\n",
    "\n",
    "tags_train, category_list = read_tag_train()\n",
    "category_list = list(set(category_list))\n",
    "\n",
    "#print tags_train   # empty space in front of each? \n",
    "#print category_list\n",
    "print len(category_list)\n",
    "print tags_train\n",
    "    \n",
    "\n",
    "# bag of words\n",
    "def process_BoW(desc_data):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    desc_words_bow = []\n",
    "    m = 0\n",
    "    for descs in desc_data:\n",
    "        temp = ''\n",
    "        for i in range(len(descs)):\n",
    "            if m == 20:\n",
    "                print descs[i]\n",
    "            word_lower = descs[i].lower()\n",
    "            if m == 20:\n",
    "                print word_lower\n",
    "            word_list = re.findall(r\"[a-z]+\", word_lower)\n",
    "            if m == 20:\n",
    "                print word_list\n",
    "            word_filter = [stemmer.stem(word) for word in word_list if word not in stopwords]\n",
    "            if m == 20:\n",
    "                print word_filter \n",
    "            for j in range (len(word_filter)):\n",
    "                temp = temp + ' '+word_filter[j] \n",
    "            if m == 20:\n",
    "                print temp \n",
    "            m = m +1\n",
    "        desc_words_bow.append(temp)\n",
    "    return desc_words_bow\n",
    "\n",
    "#dict_bow = process_BoW(desc_train)\n",
    "desc_word_bow = process_BoW(desc_train)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "cv = CountVectorizer(min_df = 5) # frequency threshold\n",
    "x_bow_train = cv.fit_transform(desc_word_bow)\n",
    "# print x_bow_train.toarray()\n",
    "words_extract = np.array(cv.get_feature_names())\n",
    "\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from yellowbrick.features import Rank1D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, figsize=(10, 14))\n",
    "visualizer = FreqDistVisualizer(features=cv.get_feature_names())\n",
    "visualizer.fit(x_bow_train)\n",
    "visualizer.poof(outpath=\"freq.png\")\n",
    "\n",
    "print len(words_extract)\n",
    "print type(words_extract)\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "x_tf_train = transformer.fit_transform(x_bow_train).toarray()\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 14))\n",
    "visualizer = FreqDistVisualizer(features=words_extract)\n",
    "visualizer.fit(x_tf_train)\n",
    "# visualizer.poof(outpath=\"freq_tfid.png\")\n",
    "\n",
    "# Convert into DataFrames\n",
    "X_train_tfidf = pd.DataFrame(x_tf_train, columns = words_extract)\n",
    "X_train_bow = pd.DataFrame(x_bow_train.toarray(), columns = words_extract)\n",
    "X_train_tfidf.head(20)\n",
    "# X_train_bow.head(20)\n",
    "\n",
    "x_tf_train = pd.DataFrame(x_tf_train, columns = words_extract)\n",
    "\n",
    "cv = CountVectorizer(vocabulary = category_list)\n",
    "tags_train_c = cv.fit_transform(tags_train).toarray()\n",
    "\n",
    "def read_des_test():\n",
    "    train_path = './data/descriptions_test/' \n",
    "    des_test = []\n",
    "    for i in range(2000):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        des = []\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                des.append(line.strip('\\n'))\n",
    "        des_test.append(des)\n",
    "    return des_test\n",
    "\n",
    "desc_test = read_des_test()\n",
    "\n",
    "desc_test_bow = process_BoW(desc_test) # To do: process BOW \n",
    "\n",
    "\n",
    "# process test data\n",
    "\n",
    "\n",
    "cv = CountVectorizer(vocabulary = words_extract)\n",
    "x_bow_test = cv.fit_transform(desc_test_bow)\n",
    "transformer = TfidfTransformer()\n",
    "x_test_tf = transformer.fit_transform(x_bow_test).toarray()\n",
    "\n",
    "# Convert into DataFrames\n",
    "x_test_tf = pd.DataFrame(x_test_tf, columns = words_extract)\n",
    "\n",
    "\n",
    "\n",
    "print len(x_test_tf)\n",
    "\n",
    "# PCA \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_pool5 = PCA(n_components=2048, svd_solver='auto') # 2048 -> change threshold \n",
    "pca_pool5.fit(x_tf_train)\n",
    "bow_all_train_PCA = pca_pool5.transform(x_tf_train)\n",
    "#bow_all_test_PCA = pca_all_pool5.transform(bow_all_test)\n",
    "\n",
    "plt.scatter(bow_all_train_PCA[:,0], bow_all_train_PCA[:,1], c=data.target)\n",
    "plt.show()\n",
    "\n",
    "pca_test = PCA(n_components=2048, svd_solver='auto') # 2048 -> change threshold \n",
    "#pca_test.fit(x_test_tf)\n",
    "test_PCA = pca_pool5.transform(x_test_tf)\n",
    "\n",
    "\n",
    "pool5_train_data = pd.read_csv(filepath_or_buffer=\"./data/features_train/features_resnet1000intermediate_train.csv\",header=None\n",
    ")\n",
    "pool5_train_data[0] = pool5_train_data[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "pool5_train_sort = pool5_train_data.sort_values(by=[0])\n",
    "pool5_train_images = pool5_train_sort[0].values\n",
    "pool5_train = pool5_train_sort[list(range(1,2049))].values\n",
    "\n",
    "# pool5 test\n",
    "pool5_test_data = pd.read_csv(filepath_or_buffer=\"./data/features_test/features_resnet1000intermediate_test.csv\", header=None\n",
    ")\n",
    "pool5_test_data[0] = pool5_test_data[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "pool5_test_sort = pool5_test_data.sort_values(by=[0])\n",
    "pool5_test_images = pool5_test_sort[0].values\n",
    "pool5_test = pool5_test_sort[list(range(1,2049))].values\n",
    "\n",
    "# from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# pls_all_pool5 = PLSRegression(n_components=1224)\n",
    "# pls_all_pool5.fit(bow_all_train_PCA, pool5_train)\n",
    "\n",
    "\n",
    "# Ramdom Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "clf = RandomForestRegressor(n_jobs=-1)\n",
    "clf.fit(bow_all_train_PCA, pool5_train)\n",
    "\n",
    "#predict_tags = []\n",
    "\"\"\"\n",
    "for i in range(80):\n",
    "    print i\n",
    "    y = tags_train_c[:,i]\n",
    "    clf = RandomForestRegressor(n_jobs=-1)\n",
    "    clf.fit(bow_all_train_PCA, pool5_train)\n",
    "    tag = clf.predict(x_test_tf)\n",
    "    predict_tags.append(tag)\n",
    "  \"\"\"  \n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbs = NearestNeighbors(n_neighbors=20, metric='cosine').fit(pool5_test)\n",
    "#np.array([test_PCA[5]])\n",
    "predict = clf.predict(np.array([test_PCA[5]]))\n",
    "distance, idx = nbs.kneighbors(predict) #success here\n",
    "\n",
    "predict_p5 = []\n",
    "for i in range(2000):\n",
    "    predict = clf.predict(np.array([test_PCA[i]]))\n",
    "    #predict_p5.append(predict)\n",
    "    distance, idx = nbs.kneighbors(predict) #success here\n",
    "    predict_p5.append(idx[0].tolist())\n",
    "\n",
    "print predict_p5\n",
    "\n",
    "\n",
    "\n",
    "predict_tags = np.array(predict_tags)\n",
    "predict_tags_t = predict_tags.T\n",
    "\n",
    "\n",
    "def read_tag_test():\n",
    "    train_path = './data/tags_test/' \n",
    "    tag_test = []\n",
    "    for i in range(2000):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        tag = ''\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                sup_cat, sub_cat = line.strip('\\n').split(':')\n",
    "                tag = tag + ' ' + sub_cat\n",
    "        tag.strip()\n",
    "        tag_test.append(tag)\n",
    "    return tag_test #, cat_list\n",
    "\n",
    "tags_test = read_tag_test()\n",
    "\n",
    "cv = CountVectorizer(vocabulary = category_list)\n",
    "tags_test_c = cv.fit_transform(tags_test)\n",
    "\n",
    "\n",
    "# knn\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "\n",
    "label_test = []\n",
    "for i in range(2000):\n",
    "    label_test.append(i)\n",
    "knn = KNN(n_neighbors = 20, n_jobs=-1)\n",
    "knn = knn.fit(tags_test_c, label_test)\n",
    "prediction = knn.kneighbors(predict_tags_t, return_distance = False)\n",
    "\n",
    "img_list = []\n",
    "for row in (predict_p5): # pool5 pca predictions \n",
    "    img_num = ''\n",
    "    for val in row:\n",
    "        img_num = img_num + ' ' + (str(val) + \".jpg\")\n",
    "    img_list.append(img_num)\n",
    "    \n",
    "images = pd.DataFrame(img_list, columns = [\"Top_20_Image_IDs\"])\n",
    "\n",
    "idx = []\n",
    "for i in range(2000):\n",
    "    idx.append(str(i)+ \".txt\")\n",
    "idx = pd.DataFrame(idx, columns = [\"Descritpion_ID\"])\n",
    "\n",
    "\n",
    "result = pd.concat([idx, images], axis=1)\n",
    "result.to_csv(\"test_p5PCA_result.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

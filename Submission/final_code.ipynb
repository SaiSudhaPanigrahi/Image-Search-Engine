{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10000\n",
    "v_len = 1000\n",
    "test_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in description txt (5 sentences each)\n",
    "def read_des_train():\n",
    "    train_path = './data/descriptions_train/' \n",
    "    des_train = []\n",
    "    for i in range(train_size):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        des = []\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                des.append(line.strip('\\n'))\n",
    "        des_train.append(des)\n",
    "    return des_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_train = read_des_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in tags\n",
    "# build diction\n",
    "def read_tag_train():\n",
    "    train_path = './data/tags_train/' \n",
    "    tag_train = []\n",
    "    cat_list = []\n",
    "    sup_cat_list = []\n",
    "    for i in range(train_size):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        tag = ''\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                sup_cat, sub_cat = line.strip('\\n').split(':')\n",
    "                tag = tag + ' ' + sub_cat\n",
    "                cat_list.append(sub_cat)\n",
    "                sup_cat_list.append(sup_cat)\n",
    "        tag.strip()\n",
    "        tag_train.append(tag)\n",
    "    return tag_train, cat_list, sup_cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_train, category_list, sup_list = read_tag_train()\n",
    "category_list = list(set(category_list))\n",
    "sup_list = list(set(sup_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process \n",
    "def preprop_description(desc_data):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    desc_words_bow = []\n",
    "    for descs in desc_data:\n",
    "        temp = ''\n",
    "        for i in range(len(descs)):\n",
    "            word_lower = descs[i].lower()\n",
    "            word_list = re.findall(r\"[a-z]+\", word_lower)\n",
    "            word_filter = [stemmer.stem(word) for word in word_list if word not in stopwords]\n",
    "            for j in range (len(word_filter)):\n",
    "                temp = temp + ' '+word_filter[j] \n",
    "        desc_words_bow.append(temp)\n",
    "    return desc_words_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_bow = preprop_description(desc_train)\n",
    "desc_word_bow = preprop_description(desc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "cv = CountVectorizer(min_df = 15) # frequency threshold\n",
    "x_bow_train = cv.fit_transform(desc_word_bow)\n",
    "words_extract = np.array(cv.get_feature_names())\n",
    "transformer = TfidfTransformer()\n",
    "x_tf_train = transformer.fit_transform(x_bow_train).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf_train = pd.DataFrame(x_tf_train, columns = words_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(vocabulary = category_list)\n",
    "tags_train_c = cv.fit_transform(tags_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_des_test():\n",
    "    train_path = './data/descriptions_test/' \n",
    "    des_test = []\n",
    "    for i in range(2000):\n",
    "        file_name = str(i) + '.txt'\n",
    "        file_path = train_path + file_name\n",
    "        des = []\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                des.append(line.strip('\\n'))\n",
    "        des_test.append(des)\n",
    "    return des_test\n",
    "\n",
    "desc_test = read_des_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_test_bow = preprop_description(desc_test) # To do: process BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test data\n",
    "\n",
    "cv = CountVectorizer(vocabulary = words_extract)\n",
    "x_bow_test = cv.fit_transform(desc_test_bow)\n",
    "transformer = TfidfTransformer()\n",
    "x_test_tf = transformer.fit_transform(x_bow_test).toarray()\n",
    "\n",
    "x_test_tf = pd.DataFrame(x_test_tf, columns = words_extract)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(x_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_pool5 = PCA(n_components=1224, svd_solver='auto') \n",
    "pca_pool5.fit(x_tf_train)\n",
    "train_PCA = pca_pool5.transform(x_tf_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_PCA = pca_pool5.transform(x_test_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature train data pool-5\n",
    "\n",
    "ft_train_data = pd.read_csv(filepath_or_buffer=\"./data/features_train/features_resnet1000intermediate_train.csv\",header=None\n",
    ")\n",
    "ft_train_data[0] = ft_train_data[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "ft_train_sort = ft_train_data.sort_values(by=[0])\n",
    "ft_train = ft_train_sort[list(range(1,2049))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2049)\n"
     ]
    }
   ],
   "source": [
    "print ft_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature test data pool-5\n",
    "ft_test_data = pd.read_csv(filepath_or_buffer=\"./data/features_test/features_resnet1000intermediate_test.csv\",header=None\n",
    ")\n",
    "ft_test_data[0] = ft_test_data[0].apply(lambda x: int(x.split('/')[1].replace('.jpg','')))\n",
    "ft_test = ft_test_data[list(range(1,2049))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLSRegression(copy=True, max_iter=500, n_components=1224, scale=True,\n",
       "       tol=1e-06)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# Partial Least Squares Regression \n",
    "pls_p5 = PLSRegression(n_components=1224) # 2048 for future approach\n",
    "pls_p5.fit(train_PCA, ft_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbs = NearestNeighbors(n_neighbors=20, metric='cosine').fit(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result \n",
    "predict_pls = []\n",
    "for i in range(2000):\n",
    "    predict = pls_p5.predict(np.array([test_PCA[i]]))\n",
    "    distance, idx = nbs.kneighbors(predict) #success here\n",
    "    predict_pls.append(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "# Ramdom Forest\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# predict_tags = []\n",
    "# for i in range(80):\n",
    "#     print i\n",
    "#     y = tags_train_c[:,i]\n",
    "#     clf = RandomForestRegressor()\n",
    "#     clf.fit(x_tf_train, y)\n",
    "#     tag = clf.predict(x_test_tf)\n",
    "#     predict_tags.append(tag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression approach\n",
    "\n",
    "lr_predict_tags = [] \n",
    "for i in range(80):\n",
    "    print i\n",
    "    y = tags_train_c[:,i]\n",
    "    clf = LinearRegression(n_jobs = -1) \n",
    "    clf.fit(x_tf_train, y)\n",
    "    tag = clf.predict(x_test_tf)\n",
    "    lr_predict_tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#train_p = preprocessing.normalize(train_feature, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "# print len(predict_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_tags = np.array(predict_tags)\n",
    "# predict_tags_t = predict_tags.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_tag_test():\n",
    "#     train_path = './data/tags_test/' \n",
    "#     tag_test = []\n",
    "#     for i in range(2000):\n",
    "#         file_name = str(i) + '.txt'\n",
    "#         file_path = train_path + file_name\n",
    "#         tag = ''\n",
    "#         with open(file_path) as f:\n",
    "#             for line in f.readlines():\n",
    "#                 sup_cat, sub_cat = line.strip('\\n').split(':')\n",
    "#                 tag = tag + ' ' + sub_cat\n",
    "#         tag.strip()\n",
    "#         tag_test.append(tag)\n",
    "#     return tag_test #, cat_list\n",
    "\n",
    "# tags_test = read_tag_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(vocabulary = category_list)\n",
    "# tags_test_c = cv.fit_transform(tags_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # knn\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors as KNN\n",
    "\n",
    "# label_test = []\n",
    "# for i in range(2000):\n",
    "#     label_test.append(i)\n",
    "# knn = KNN(n_neighbors = 20)\n",
    "# knn = knn.fit(tags_test_c, label_test)\n",
    "# prediction = knn.kneighbors(predict_tags_t, return_distance = False)”“”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for row in (predict_pls):\n",
    "    img_num = ''\n",
    "    for val in row:\n",
    "        img_num = img_num + ' ' + (str(val) + \".jpg\")\n",
    "    img_list.append(img_num)\n",
    "idx = []\n",
    "for i in range(2000):\n",
    "    idx.append(str(i)+ \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv file\n",
    "images = pd.DataFrame(img_list, columns = [\"Top_20_Image_IDs\"])\n",
    "idx = pd.DataFrame(idx, columns = [\"Descritpion_ID\"])\n",
    "result = pd.concat([idx, images], axis=1)\n",
    "result.to_csv(\"pls_pcaP5_result.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
